{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravilladivyaUNT/Divya_INFO5731_-Spring2022/blob/main/In_CLASS_exercise_04_2_(1)_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la3aIynmA3ag"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A28S5V42A3ai"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIFT7KV1A3ai"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPJFC_1A3aj"
      },
      "source": [
        "packages used in this TASK are re, gensim, spacy and pyLDAvis. Besides this we will also using matplotlib, numpy and pandas for data handling and visualization. Let’s import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU9_37XiA3aj",
        "outputId": "91c6ac97-f920-4f01-c5a7-a82891cb684d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.4.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.11.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.21.5)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyldavis) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyldavis) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyldavis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyldavis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyldavis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyldavis) (3.0.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyldavis) (3.1.0)\n",
            "Building wheels for collected packages: pyldavis\n",
            "  Building wheel for pyldavis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=cc2399e609d56c73914369f51dbf006363f393d13180edb4d35372a533c481e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyldavis\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-1.17 pyldavis-3.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "!pip install pyldavis\n",
        "import pyLDAvis\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ4cXlkMA3al"
      },
      "source": [
        "Since the goal of this analysis is to perform topic modeling, let’s focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we’ll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knmtWJhwA3al"
      },
      "outputs": [],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOjz94REA3am"
      },
      "source": [
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7M8SjJeA3an"
      },
      "outputs": [],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISX-zskA3an"
      },
      "source": [
        "To verify whether the preprocessing, we’ll make a word cloud using the wordcloud package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cox31vanA3ao"
      },
      "outputs": [],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sqfEJHCA3ao"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXM8ey9pA3ap"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "#import modules\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('..')\n",
        "# Read data into papers\n",
        "papers = pd.read_csv('./data/NIPS Papers/papers.csv')\n",
        "# Print head\n",
        "papers.head()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtEq7pYGA3ap"
      },
      "source": [
        "Since the goal of this analysis is to perform topic modeling, let’s focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we’ll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H14GpIbPA3ap"
      },
      "outputs": [],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqikShxTA3aq"
      },
      "source": [
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHx7LeuyA3aq"
      },
      "outputs": [],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtVkOE__A3aq"
      },
      "source": [
        "To verify whether the preprocessing, we’ll make a word cloud using the wordcloud package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CivPbcbLA3aq"
      },
      "outputs": [],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBTL8jAxA3ar"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDWuowFtA3at"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "from lda2vec import preprocess, Corpus\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import seaborn\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhbQx_W3A3at"
      },
      "source": [
        "Here we can use very recent version of pyLDAvis to use the lda2vec outputs. As of this writing, 16 or this commit 14e7b5f60d8360eb84969f3f08a1b77b365a5878e should work. You can do this quickly by installing it directly from master like so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bvnIAU4A3au"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_rgt8gA3au"
      },
      "source": [
        "Now aftwrunnning lda2vec_run.py script in examples/twenty_newsgroups/lda2vec directory a topics.pyldavis.npz will be created that contains the topic-to-word probabilities and frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZzvBnerA3au"
      },
      "outputs": [],
      "source": [
        "npz = np.load(open('topics.pyldavis.npz', 'r'))\n",
        "dat = {k: v for (k, v) in npz.iteritems()}\n",
        "dat['vocab'] = dat['vocab'].tolist()\n",
        "# dat['term_frequency'] = dat['term_frequency'] * 1.0 / dat['term_frequency'].sum()\n",
        "top_n = 10\n",
        "topic_to_topwords = {}\n",
        "for j, topic_to_word in enumerate(dat['topic_term_dists']):\n",
        "    top = np.argsort(topic_to_word)[::-1][:top_n]\n",
        "    msg = 'Topic %i '  % j\n",
        "    top_words = [dat['vocab'][i].strip()[:35] for i in top]\n",
        "    msg += ' '.join(top_words)\n",
        "    print msg\n",
        "    topic_to_topwords[j] = top_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtBXzuIA3av"
      },
      "source": [
        "Topic 0 x11r5 xv window xterm server motif font xlib // sunos\n",
        "Topic 1 jesus son father matthew sin mary g'd disciples christ sins\n",
        "Topic 2 s1 nsa s2 clipper chip administration q escrow private sector serial number encryption technology\n",
        "Topic 3 leafs games playoffs hockey game players pens yankees bike phillies\n",
        "Topic 4 van - 0 pp en 1 njd standings 02 6\n",
        "Topic 5 out_of_vocabulary out_of_vocabulary anonymity hiv homicide adl ripem bullock encryption technology eff\n",
        "Topic 6 hiv magi prof erzurum venus van 2.5 million ankara satellite launched\n",
        "Topic 7 nsa escrow clipper chip encryption government phones warrant vat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyRluLphA3av"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "prepared_data = pyLDAvis.prepare(dat['topic_term_dists'], dat['doc_topic_dists'], \n",
        "                                 dat['doc_lengths'] * 1.0, dat['vocab'], dat['term_frequency'] * 1.0, mds='tsne')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqzCh0ZfA3av"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "remove=('headers', 'footers', 'quotes')\n",
        "texts = fetch_20newsgroups(subset='train', remove=remove).data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWmArHr_A3av"
      },
      "source": [
        "I will be summarizing in the next two days, so please add to the network\n",
        "knowledge base if you have done the clock upgrade and haven't answered this\n",
        "poll. Thanks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J6wqklWA3av"
      },
      "outputs": [],
      "source": [
        "msg = \"{weight:02d}% in topic {topic_id:02d} which has top words {text:s}\"\n",
        "for topic_id, weight in enumerate(dat['doc_topic_dists'][1]):\n",
        "    if weight > 0.01:\n",
        "        text = ', '.join(topic_to_topwords[topic_id])\n",
        "        print msg.format(topic_id=topic_id, weight=int(weight * 100.0), text=text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gIz3cjlA3av"
      },
      "source": [
        "03% in topic 00 which has top words x11r5, xv, window, xterm, server, motif, font, xlib, //, sunos\n",
        "03% in topic 03 which has top words leafs, games, playoffs, hockey, game, players, pens, yankees, bike, phillies\n",
        "22% in topic 08 which has top words mac, controller, shipping, disk, printer, mb, ethernet, enable, os/2, port\n",
        "41% in topic 13 which has top words mac, i, thanks, monitor, apple, upgrade, card, connect, using, windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9B-UHnGA3av"
      },
      "outputs": [],
      "source": [
        "plt.bar(np.arange(20), dat['doc_topic_dists'][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlQT9VXrA3aw"
      },
      "source": [
        "<Container object of 20 artists>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHaTpeyMA3aw"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccyXbWC8A3aw"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkKl9LGYA3aw"
      },
      "source": [
        " We set language to english since our documents are in the English language. If you would like to use a multi-lingual model, please use language=\"multilingual\" instead.\n",
        "\n",
        "We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzCo4aB5A3aw"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeEO5sVIA3aw"
      },
      "source": [
        "After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBlU8kguA3aw"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info(); freq.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__kZMbS4A3ax"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info(); freq.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg6h4WnkA3ax"
      },
      "source": [
        "Topic\tCount\tName\n",
        "0\t-1\t8633\t-1_file_use_any_me\n",
        "1\t0\t828\t0_jesus_bible_faith_christ\n",
        "2\t1\t708\t1_space_nasa_orbit_spacecraft\n",
        "3\t2\t625\t2_game_hockey_nhl_leafs\n",
        "4\t3\t600\t3_baseball_pitching_pitcher_cubs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY073z51A3ax"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(0)  # Select the most frequent topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvupvXW8A3ax"
      },
      "source": [
        "After having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuz8daksA3ax"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx-OARpQA3ax"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_term_rank()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfGbXU8jA3ax"
      },
      "source": [
        "When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stopwords or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PaOH8FWA3ax"
      },
      "outputs": [],
      "source": [
        "opic_model.update_topics(docs, topics, n_gram_range=(1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ9AiILxA3ax"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gApHvfggA3ax"
      },
      "source": [
        "\n",
        "For all cases, the similar documents are more apparent in the top and bottom groups of100 patent documents.\n",
        " The fact that both methods agree on the most and least similardocuments can help designers to look at the two groups for near-field or far-field\n",
        "analogies. Depending on the goal of the designer, they can analyze similar documentsor dissimilar documents during design ideationIn LDA, the two probability distributions, p(z|d) and p(w|z), are assumed to be multinomial distributionsThus, the topic distributions in all documents share the common Dirichlet prior αα, and the word distributions of topics share the common Dirichlet prior ηη. Given the parameters αα and ηη for document d, parameter θ d of a multinomial distribution over K topics is constructed from Dirichlet distribution Dir(θ d |α). \n",
        "Similarly, for topic k, parameter β k of a multinomial distribution over V words is derived from Dirichlet distribution Dir(β k |η). As a conjugate prior for the multinomial, the Dirichlet distribution is a convenient choice as a prior and can simplify the statistical inference in LDA. Therefore, in PLSA, by contrast, any common prior probability distribution was not specified for p(z|d) andp(w|z). Naturally, there are no αα and ηη in the generative process of LSA.\n",
        "As an unsupervised learning model, LDA can discover underlying topics in unlabeled data. Nevertheless, “topics” discovered in an unsupervised way may not match the true topics in the data. Therefore, many researchers modified LDA in a supervised learning manner, which can introduce known label information into the topic discovery process.\n",
        "\n",
        "The typical supervised topic models include supervised LDA  the discriminative variation on LDA (discLDA) (Lacoste-Julien et al. 2009), and maximum entropy discrimination LDA (medLDA) (Zhu et al. 2012). For example, sLDA associates each document with an observable continuous response variable, and models the response variables using normal linear regression. A multilabel topic model called labeled LDA (LLDA) (Ramage et al. 2009) extends previous supervised models to allow for multiple labels of documents, and the relation of labels to topics represents one-to-one mapping. Partially labeled LDA (PLLDA) (Ramage et al. 2011) further extends LLDA to have latent topics not present in the document labels.\n",
        "lda2vec specifically builds on top of the skip-gram model of word2vec to generate word vectors. If you’re not familiar with skip-gram and word2vec, you can read up on it here, but essentially it’s a neural net that learns a word embedding by trying to use the input word to predict surrounding context words.\n",
        "BEST MODEL ALGO\n",
        "LDA Latent Dirichlet Allocation: Based on the Bayesian approach of describing all forms of statistical uncertainties in probabilities, LDA or Latent Dirichlet Allocation depicts an infinite mixture of topics probabilities that are represented in a document.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_CLASS_exercise_04-2 (1) Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}